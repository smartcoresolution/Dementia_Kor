{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 치매 음성 데이터 분류 프로젝트\n",
    "\n",
    "이 노트북은 치매 관련 음성 데이터를 기반으로 CNN, ViT, RandomForest 모델을 활용해 분류 작업을 수행합니다. \n",
    "코드는 환경설정, 데이터 준비, 모델 정의, 학습/평가/시각화의 구조로 이루어져 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ==== 환경설정 및 패키지 설치 ====\n",
    "# Colab 환경에서 사용 시 아래 셀을 실행하세요. 로컬 환경은 각 패키지 별도 설치 필요.\n",
    "# !pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 scikit-learn==1.3.2 transformers==4.40.1 librosa==0.10.1 matplotlib==3.7.1\n",
    "\n",
    "import os, random, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import librosa\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "AUDIO_MAX_LEN = 16000*10  # 10초 기준 패딩\n",
    "N_MELS = 128\n",
    "DATA_DIR = '/content/drive/MyDrive/your_data_dir'  # 데이터 위치로 변경\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ==== 데이터 준비 및 경로/레이블 수집 ====\n",
    "def get_audio_paths_and_labels(data_dir):\n",
    "    paths, labels = [], []\n",
    "    for label_dir in os.listdir(data_dir):\n",
    "        label_path = os.path.join(data_dir, label_dir)\n",
    "        if os.path.isdir(label_path):\n",
    "            for fname in os.listdir(label_path):\n",
    "                if fname.endswith('.wav'):\n",
    "                    paths.append(os.path.join(label_path, fname))\n",
    "                    labels.append(0 if label_dir == 'normal' else 1)  # normal=0, dementia=1\n",
    "    return paths, labels\n",
    "\n",
    "audio_paths, audio_labels = get_audio_paths_and_labels(DATA_DIR)\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    audio_paths, audio_labels, test_size=0.2, random_state=SEED, stratify=audio_labels\n",
    ")\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, paths, labels, mode='cnn', feature_extractor=None, augment=False):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        y, sr = librosa.load(path, sr=16000)\n",
    "        if len(y) < AUDIO_MAX_LEN:\n",
    "            y = np.pad(y, (0, AUDIO_MAX_LEN - len(y)))\n",
    "        else:\n",
    "            y = y[:AUDIO_MAX_LEN]\n",
    "        if self.augment:\n",
    "            y = y + np.random.normal(0, 0.005, size=y.shape)\n",
    "        mel = librosa.feature.melspectrogram(y, sr=sr, n_mels=N_MELS)\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "        mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n",
    "        if self.mode == 'cnn':\n",
    "            return torch.tensor(mel_db, dtype=torch.float32).unsqueeze(0), torch.tensor(label, dtype=torch.long)\n",
    "        elif self.mode == 'vit':\n",
    "            image = np.stack([mel_db]*3, axis=0)\n",
    "            if self.feature_extractor:\n",
    "                image = self.feature_extractor(images=image.transpose(1,2,0), return_tensors='pt')['pixel_values'][0]\n",
    "            else:\n",
    "                image = torch.tensor(image, dtype=torch.float32)\n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "        elif self.mode == 'raw':\n",
    "            return mel_db.flatten(), label"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ==== 모델 정의 (CNN, ViT, RandomForest) ====\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, n_mels=N_MELS, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, (5,5), stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3,3), stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, (3,3), stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64*(n_mels//8)*(mel_db.shape[1]//8), 128)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.flatten(1)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ViT: HuggingFace 모델 활용\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=2)\n",
    "\n",
    "# RandomForest는 scikit-learn 사용"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ==== 학습, 평가, 시각화 함수 및 전체 실행 ====\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
    "    model.to(device)\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # 검증\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                preds = outputs.argmax(1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        val_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}, Val Accuracy: {val_acc:.4f}\")\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "def eval_model(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            outputs = model(x)\n",
    "            probs = nn.functional.softmax(outputs, dim=1)\n",
    "            preds = outputs.argmax(1).cpu().numpy()\n",
    "            y_true.extend(y.numpy())\n",
    "            y_pred.extend(preds)\n",
    "            y_prob.extend(probs.cpu().numpy())\n",
    "    return np.array(y_true), np.array(y_pred), np.array(y_prob)\n",
    "\n",
    "def train_rf(train_dataset, val_dataset):\n",
    "    X_train = np.array([train_dataset[i][0] for i in range(len(train_dataset))])\n",
    "    y_train = np.array([train_dataset[i][1] for i in range(len(train_dataset))])\n",
    "    X_val = np.array([val_dataset[i][0] for i in range(len(val_dataset))])\n",
    "    y_val = np.array([val_dataset[i][1] for i in range(len(val_dataset))])\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=SEED)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_val)\n",
    "    y_prob = rf.predict_proba(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_prob[:,1])\n",
    "    print(f\"RF ValAcc={acc:.4f}, ROC-AUC={auc:.4f}\")\n",
    "    return y_val, y_pred, y_prob\n",
    "\n",
    "def plot_results(y_true, y_prob, title):\n",
    "    if len(y_true) == 0 or len(y_prob) == 0:\n",
    "        print(\"No data for plotting.\")\n",
    "        return\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:,1])\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'{title} ROC curve (AUC={roc_auc_score(y_true, y_prob[:,1]):.2f}')\n",
    "    plt.plot([0,1],[0,1],'--',color='gray')\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(title); plt.legend(); plt.show()\n",
    "\n",
    "# --- CNN ---\n",
    "train_cnn_ds = AudioDataset(train_paths, train_labels, mode='cnn', augment=True)\n",
    "val_cnn_ds = AudioDataset(val_paths, val_labels, mode='cnn')\n",
    "train_cnn_loader = DataLoader(train_cnn_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_cnn_loader = DataLoader(val_cnn_ds, batch_size=BATCH_SIZE)\n",
    "cnn_model = SimpleCNN(n_mels=N_MELS)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
    "train_model(cnn_model, train_cnn_loader, val_cnn_loader, criterion, optimizer, EPOCHS, DEVICE)\n",
    "y_true_cnn, y_pred_cnn, y_prob_cnn = eval_model(cnn_model, val_cnn_loader, DEVICE)\n",
    "plot_results(y_true_cnn, y_prob_cnn, \"CNN\")\n",
    "\n",
    "# --- ViT ---\n",
    "train_vit_ds = AudioDataset(train_paths, train_labels, mode='vit', feature_extractor=feature_extractor, augment=True)\n",
    "val_vit_ds = AudioDataset(val_paths, val_labels, mode='vit', feature_extractor=feature_extractor)\n",
    "train_vit_loader = DataLoader(train_vit_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_vit_loader = DataLoader(val_vit_ds, batch_size=BATCH_SIZE)\n",
    "optimizer_vit = optim.Adam(vit_model.parameters(), lr=LEARNING_RATE)\n",
    "train_model(vit_model, train_vit_loader, val_vit_loader, criterion, optimizer_vit, EPOCHS, DEVICE)\n",
    "y_true_vit, y_pred_vit, y_prob_vit = eval_model(vit_model, val_vit_loader, DEVICE)\n",
    "plot_results(y_true_vit, y_prob_vit, \"ViT\")\n",
    "\n",
    "# --- RandomForest ---\n",
    "train_rf_ds = AudioDataset(train_paths, train_labels, mode='raw', augment=True)\n",
    "val_rf_ds = AudioDataset(val_paths, val_labels, mode='raw')\n",
    "y_true_rf, y_pred_rf, y_prob_rf = train_rf(train_rf_ds, val_rf_ds)\n",
    "plot_results(y_true_rf, y_prob_rf, \"RandomForest\")\n",
    "\n",
    "# --- 전체 성능 비교 리포트 ---\n",
    "def print_report(name, y_true, y_pred):\n",
    "    print(f\"\\n{name} Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_true, y_pred)}\")\n",
    "\n",
    "print_report(\"CNN\", y_true_cnn, y_pred_cnn)\n",
    "print_report(\"ViT\", y_true_vit, y_pred_vit)\n",
    "print_report(\"RandomForest\", y_true_rf, y_pred_rf)"
   ]
  }
 ]
}